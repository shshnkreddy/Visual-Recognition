{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from collections import Counter\n",
    "import pickle\n",
    "import torch_geometric\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch import nn\n",
    "from torch_geometric.nn import GATConv, global_mean_pool\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import math\n",
    "from torch_geometric.data import Data, DataLoader\n",
    "import random\n",
    "import time\n",
    "import nltk\n",
    "from earlystopping import EarlyStopping\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.decomposition import PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('Flickr8k.token.txt', sep = \"\\t\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded_features = np.load('googlenet_1024_features.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = PCA(n_components=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded_features = pca.fit_transform(encoded_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.download('averaged_perceptron_tagger')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(5):\n",
    "    df = df[df['FileName'] != '2258277193_586949ec62.jpg.1#' + str(i)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preProcess(s):\n",
    "    s = s.lower()\n",
    "    \n",
    "    # initializing punctuations string\n",
    "    punc = '''!()-[]{};:'\"\\, <>./?@#$%^&*_~'''\n",
    "    # Removing punctuations in string\n",
    "    # Using loop + punctuation string\n",
    "    for ele in s: \n",
    "        if ele in punc: \n",
    "            s = s.replace(ele, \" \")\n",
    "    s = s.split()\n",
    "    l = []\n",
    "    banned = ['IN', 'DT']\n",
    "    for i in s:\n",
    "        #print(i)\n",
    "        if(nltk.pos_tag([i])[0][1] not in banned):\n",
    "            l.append(i)\n",
    "            #print(i)\n",
    "    return l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.pos_tag(['is'])[0][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preProcess(\"for\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.Description = df.Description.apply(preProcess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.Description"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mp = {}\n",
    "for idx, rows in df.iterrows():\n",
    "    fname = rows['FileName'][:-2]\n",
    "    if(fname not in mp):\n",
    "        mp[fname] = []\n",
    "    mp[fname].append(rows['Description'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(mp.items(), columns = ['FileName', 'Description'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "counts = {}\n",
    "for idx, rows in df.iterrows():\n",
    "    for j in rows['Description']:\n",
    "        for k in j:\n",
    "            if(k not in counts):\n",
    "                counts[k] = 1\n",
    "            else:\n",
    "                counts[k] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "common_words = Counter(counts).most_common(5000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "map_vocab = {}\n",
    "cnt = 1\n",
    "for i in common_words:\n",
    "    map_vocab[i[0]] = cnt\n",
    "    cnt += 1\n",
    "map_vocab['UNK'] = cnt\n",
    "cnt += 1\n",
    "map_vocab[\"SOF\"] = cnt\n",
    "cnt += 1\n",
    "map_vocab[\"EOF\"] = cnt\n",
    "map_vocab[\"Padding\"] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inv_mapping = {}\n",
    "for i in map_vocab:\n",
    "    inv_mapping[map_vocab[i]] = i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "counts['UNK'] = counts['man']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights = [0]\n",
    "for i in range(1, 5002):\n",
    "    x = np.log(1/counts[inv_mapping[i]]) + 12\n",
    "    weights.append(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights.append(0)\n",
    "weights.append(np.log(1/40000) + 12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "VOCAB_SIZE = 5004\n",
    "\n",
    "SEQ_LEN = 60"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convertVocab(x):\n",
    "    l = []\n",
    "    for i in x:\n",
    "        m = []\n",
    "        m.append(map_vocab['SOF'])\n",
    "        for j in i:\n",
    "            #print(j)\n",
    "            if(j in map_vocab):\n",
    "                m.append(map_vocab[j])\n",
    "            else:\n",
    "                m.append(map_vocab['UNK'])\n",
    "        m = m[:min(len(m), SEQ_LEN - 1)]\n",
    "        m.append(map_vocab['EOF'])\n",
    "        l.append(m)\n",
    "    return l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.Description = df.Description.apply(convertVocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = []\n",
    "for idx, rows in df.iterrows():\n",
    "    curr_x = torch.tensor(encoded_features[idx])\n",
    "    for j in rows['Description']:\n",
    "        #j = j[:min(len(j), SEQ_LEN)]\n",
    "        if(len(j) < SEQ_LEN):\n",
    "            j = torch.cat([torch.tensor(j), torch.zeros(SEQ_LEN - len(j))])\n",
    "        else:\n",
    "            j = torch.tensor(j)\n",
    "        train_data.append((curr_x, j.long()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "split = int(len(train_data) * 80 / 100)\n",
    "validation_data = train_data[split:]\n",
    "train_data = train_data[:split]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 64\n",
    "train_loader = DataLoader(train_data, BATCH_SIZE, shuffle=True, num_workers=4, pin_memory=True)\n",
    "val_loader = DataLoader(validation_data, BATCH_SIZE, shuffle = True, num_workers = 4, pin_memory = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "WORD_EMBEDDING_DIM = 300\n",
    "HIDDEN_SIZE = 64\n",
    "\n",
    "class Decoder(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.embeddingLayer = nn.Embedding(VOCAB_SIZE, WORD_EMBEDDING_DIM)\n",
    "        self.dropout = nn.Dropout(.5)\n",
    "        self.GRU = nn.GRU(input_size = WORD_EMBEDDING_DIM, hidden_size = HIDDEN_SIZE, \n",
    "                          batch_first = True, num_layers = 1)\n",
    "        self.linear = nn.Linear(in_features = HIDDEN_SIZE, out_features = VOCAB_SIZE)\n",
    "\n",
    "    def forward(self, x, h_0):\n",
    "        x = x.to(device)\n",
    "        h_0 = h_0.to(device)\n",
    "        #print(x.shape)\n",
    "        x = self.embeddingLayer(x)\n",
    "        x = self.dropout(x)\n",
    "        #print(x.shape)\n",
    "        x = x.view((BATCH_SIZE, 1, -1))\n",
    "        #print(x.shape)\n",
    "        h_0 = h_0.view(1, BATCH_SIZE, HIDDEN_SIZE)\n",
    "        _, h_n = self.GRU(x, h_0)\n",
    "        h_n = h_n.view((BATCH_SIZE, -1))\n",
    "        out = self.linear(h_n)\n",
    "        return out, h_n\n",
    "    \n",
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Decoder()\n",
    "device = torch.device('cuda:0')\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cnt = 0\n",
    "# for batch in train_loader:\n",
    "#     x = batch[1]\n",
    "#     h = batch[0].float()\n",
    "#     if(x.shape[0] != BATCH_SIZE):\n",
    "#         print(x.shape[0])\n",
    "#         continue\n",
    "#     for j in range(SEQ_LEN - 1):\n",
    "#         out, h = model(x[:, j], h)\n",
    "#         y_true = x[:, j + 1].to(device)\n",
    "#         print(y_true.shape)\n",
    "#         criterion(out, y_true)\n",
    "# print(cnt)        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights = torch.tensor(weights)\n",
    "weights += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights = weights.to(device).float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = torch.nn.CrossEntropyLoss(ignore_index = 0, weight = weights)\n",
    "optimizer = torch.optim.Adam(model.parameters())\n",
    "\n",
    "def train_model(model, patience = 3, n_epochs = 20):\n",
    "    \n",
    "    # to track the training loss as the model trains\n",
    "    train_losses = []\n",
    "    # to track the validation loss as the model trains\n",
    "    valid_losses = []\n",
    "    # to track the average training loss per epoch as the model trains\n",
    "    avg_train_losses = []\n",
    "    # to track the average validation loss per epoch as the model trains\n",
    "    avg_valid_losses = [] \n",
    "    \n",
    "    # initialize the early_stopping object\n",
    "    early_stopping = EarlyStopping(patience=patience, verbose=True)\n",
    "    \n",
    "    for epoch in range(1, n_epochs + 1):\n",
    "\n",
    "        ###################\n",
    "        # train the model #\n",
    "        ###################\n",
    "        model.train() # prep model for training\n",
    "        for batch in train_loader:\n",
    "            loss = 0\n",
    "            #batch.to(device)\n",
    "            # clear the gradients of all optimized variables\n",
    "            optimizer.zero_grad()\n",
    "            # forward pass: compute predicted outputs by passing inputs to the model\n",
    "            \n",
    "            x = batch[1]\n",
    "            h = batch[0].float()\n",
    "            if(x.shape[0] != BATCH_SIZE):\n",
    "                continue\n",
    "            for j in range(SEQ_LEN - 1):\n",
    "                out, h = model(x[:, j], h)\n",
    "                y_true = x[:, j + 1].to(device)\n",
    "                loss += criterion(out, y_true)\n",
    "            #output = model(batch.x, batch.edge_index, batch.batch)\n",
    "            # calculate the loss\n",
    "            #loss = criterion(output, batch.y)\n",
    "            # backward pass: compute gradient of the loss with respect to model parameters\n",
    "            loss.backward()\n",
    "            # perform a single optimization step (parameter update)\n",
    "            optimizer.step()\n",
    "            # record training loss\n",
    "            train_losses.append(loss.item())\n",
    "\n",
    "        ######################    \n",
    "        # validate the model #\n",
    "        ######################\n",
    "        model.eval() # prep model for evaluation\n",
    "        for batch in val_loader:\n",
    "            #batch.to(device)\n",
    "            # forward pass: compute predicted outputs by passing inputs to the model\n",
    "            loss = 0\n",
    "            x = batch[1]\n",
    "            h = batch[0].float()\n",
    "            if(x.shape[0] != BATCH_SIZE):\n",
    "                continue\n",
    "            for j in range(SEQ_LEN - 1):\n",
    "                out, h = model(x[:, j], h)\n",
    "                y_true = x[:, j + 1].to(device)\n",
    "                loss += criterion(out, y_true)\n",
    "            \n",
    "            \n",
    "            #output = model(batch.x, batch.edge_index, batch.batch)\n",
    "            # calculate the loss\n",
    "            #loss = criterion(output, batch.y)\n",
    "            # record validation loss\n",
    "            valid_losses.append(loss.item())\n",
    "\n",
    "        # print training/validation statistics \n",
    "        # calculate average loss over an epoch\n",
    "        train_loss = np.average(train_losses)\n",
    "        valid_loss = np.average(valid_losses)\n",
    "        avg_train_losses.append(train_loss)\n",
    "        avg_valid_losses.append(valid_loss)\n",
    "        \n",
    "        epoch_len = len(str(n_epochs))\n",
    "        \n",
    "        print_msg = (f'[{epoch:>{epoch_len}}/{n_epochs:>{epoch_len}}] ' +\n",
    "                     f'train_loss: {train_loss:.5f} ' +\n",
    "                     f'valid_loss: {valid_loss:.5f}')\n",
    "        \n",
    "        print(print_msg)\n",
    "        \n",
    "        # clear lists to track next epoch\n",
    "        train_losses = []\n",
    "        valid_losses = []\n",
    "        \n",
    "        # early_stopping needs the validation loss to check if it has decresed, \n",
    "        # and if it has, it will make a checkpoint of the current model\n",
    "        early_stopping(valid_loss, model)\n",
    "        \n",
    "        if early_stopping.early_stop:\n",
    "            print(\"Early stopping\")\n",
    "            break\n",
    "        \n",
    "    # load the last checkpoint with the best model\n",
    "    model.load_state_dict(torch.load('checkpoint.pt'))\n",
    "\n",
    "    return  model, avg_train_losses, avg_valid_losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = train_model(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = model[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_features = np.load('googlenet_test_features.npy')\n",
    "encoded_features = np.load('googlenet_1024_features.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_features = np.vstack((test_features, encoded_features))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_features = pca.fit_transform(test_features)\n",
    "test_features = test_features[:5]\n",
    "test_features.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_features = torch.tensor(test_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_features.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.tensor(map_vocab['SOF'])\n",
    "x = x.view((1, -1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "token = 'SOF'\n",
    "cnt = 0\n",
    "h = test_features[4]\n",
    "h = h.view((1, -1)).float()\n",
    "s = \"\"\n",
    "while(token != 'EOF' and cnt < 130):\n",
    "    cnt += 1\n",
    "    #print(x.shape)\n",
    "    #print(h.shape)\n",
    "    out, h = model(x, h)\n",
    "    char = torch.argmax(out)\n",
    "    token = inv_mapping[char.item()]\n",
    "    s += token + \" \"\n",
    "print(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}